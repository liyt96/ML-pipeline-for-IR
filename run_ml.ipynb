{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrel_dict_labeled = {}\n",
    "with open('./info/qrel_dict_labeled.txt', 'r') as f:\n",
    "    qrel_dict_labeled = json.load(f)\n",
    "    \n",
    "qrel_dict_all = {}\n",
    "with open('./info/qrel_dict_all.txt', 'r') as f:\n",
    "    qrel_dict_all = json.load(f)\n",
    "    \n",
    "qrel_train = {}\n",
    "with open('./info/qrel_train.txt', 'r') as f:\n",
    "    qrel_train = json.load(f)\n",
    "\n",
    "qrel_test = {}\n",
    "with open('./info/qrel_test.txt', 'r') as f:\n",
    "    qrel_test = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_train = {}\n",
    "matrix_test = {}\n",
    "for query, label_map in qrel_dict_labeled.items():\n",
    "    if query in qrel_train:\n",
    "        for doc, label in label_map.items():\n",
    "            entry = query + '-' + doc\n",
    "            matrix_train[entry] = {}\n",
    "            matrix_train[entry]['label'] = label\n",
    "    \n",
    "for query, doc_list in qrel_dict_all.items():\n",
    "    if query in qrel_test:\n",
    "        for doc in doc_list:\n",
    "            entry = query + '-' + doc\n",
    "            matrix_test[entry] = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load train scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "okapi_tf_train_file = open('./result/okapi_tf_train.txt', 'r')\n",
    "for line in okapi_tf_train_file:\n",
    "    entry = line.split()[0] + '-' + line.split()[2]\n",
    "    matrix_train[entry]['okapi_tf'] = round(float(line.split()[4]), 4)\n",
    "    \n",
    "tf_idf_train_file = open('./result/tf_idf_train.txt', 'r')\n",
    "for line in tf_idf_train_file:\n",
    "    entry = line.split()[0] + '-' + line.split()[2]\n",
    "    matrix_train[entry]['tf_idf'] = round(float(line.split()[4]), 4)\n",
    "\n",
    "bm25_train_file = open('./result/bm25_train.txt', 'r')\n",
    "for line in bm25_train_file:\n",
    "    entry = line.split()[0] + '-' + line.split()[2]\n",
    "    matrix_train[entry]['bm25'] = round(float(line.split()[4]), 4)\n",
    "\n",
    "unigram_laplace_train_file = open('./result/unigram_laplace_train.txt', 'r')\n",
    "for line in unigram_laplace_train_file:\n",
    "    entry = line.split()[0] + '-' + line.split()[2]\n",
    "    matrix_train[entry]['unigram_laplace'] = round(float(line.split()[4]), 4)\n",
    "    \n",
    "unigram_jm_train_file = open('./result/unigram_jm_train.txt', 'r')\n",
    "for line in unigram_jm_train_file:\n",
    "    entry = line.split()[0] + '-' + line.split()[2]\n",
    "    matrix_train[entry]['unigram_jm'] = round(float(line.split()[4]), 4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "okapi_tf_test_file = open('./result/okapi_tf_test.txt', 'r')\n",
    "for line in okapi_tf_test_file:\n",
    "    entry = line.split()[0] + '-' + line.split()[2]\n",
    "    matrix_test[entry]['okapi_tf'] = round(float(line.split()[4]), 4)\n",
    "    \n",
    "tf_idf_test_file = open('./result/tf_idf_test.txt', 'r')\n",
    "for line in tf_idf_test_file:\n",
    "    entry = line.split()[0] + '-' + line.split()[2]\n",
    "    matrix_test[entry]['tf_idf'] = round(float(line.split()[4]), 4)\n",
    "\n",
    "bm25_test_file = open('./result/bm25_test.txt', 'r')\n",
    "for line in bm25_test_file:\n",
    "    entry = line.split()[0] + '-' + line.split()[2]\n",
    "    matrix_test[entry]['bm25'] = round(float(line.split()[4]), 4)\n",
    "\n",
    "unigram_laplace_test_file = open('./result/unigram_laplace_test.txt', 'r')\n",
    "for line in unigram_laplace_test_file:\n",
    "    entry = line.split()[0] + '-' + line.split()[2]\n",
    "    matrix_test[entry]['unigram_laplace'] = round(float(line.split()[4]), 4)\n",
    "    \n",
    "unigram_jm_test_file = open('./result/unigram_jm_test.txt', 'r')\n",
    "for line in unigram_jm_test_file:\n",
    "    entry = line.split()[0] + '-' + line.split()[2]\n",
    "    matrix_test[entry]['unigram_jm'] = round(float(line.split()[4]), 4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./info/matrix_train.txt', 'w') as f:\n",
    "    json.dump(matrix_train, f)\n",
    "\n",
    "with open('./info/matrix_test.txt', 'w') as f:\n",
    "    json.dump(matrix_test, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature_array = []\n",
    "train_label_array = []\n",
    "train_entry_array = []\n",
    "\n",
    "for sample, attributes in matrix_train.items():\n",
    "    train_entry_array.append(sample)\n",
    "    train_feature_array.append([ attributes['bm25'], attributes['tf_idf'], attributes['okapi_tf'], attributes['unigram_laplace'], attributes['unigram_jm'] ])\n",
    "    train_label_array.append(attributes['label'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_array = []\n",
    "test_label_array = []\n",
    "test_entry_array = []\n",
    "\n",
    "for sample, attributes in matrix_test.items():\n",
    "    test_entry_array.append(sample)\n",
    "    test_feature_array.append([ attributes['bm25'], attributes['tf_idf'], attributes['okapi_tf'], attributes['unigram_laplace'], attributes['unigram_jm'] ])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/scipy/linalg/basic.py:1226: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(train_feature_array,train_label_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lm.predict(test_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_queries = []\n",
    "for qno in qrel_test:\n",
    "    test_queries.append(qno)\n",
    "\n",
    "train_queries = []\n",
    "for qno in qrel_train:\n",
    "    train_queries.append(qno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_map = {}\n",
    "for query in test_queries:\n",
    "    test_pred_map[str(query)] = []\n",
    "    \n",
    "for i in range(len(test_entry_array)):\n",
    "    query, docno = test_entry_array[i].split('-', maxsplit=1)\n",
    "    test_pred_map[query].append([docno, predictions[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_scores(query, ml_scores, out):\n",
    "    sorted_scores = sorted(ml_scores, key=lambda x: x[1], reverse=True)\n",
    "    i = 0\n",
    "    for j in range(len(sorted_scores)):\n",
    "        if i == 1000:\n",
    "            break\n",
    "        str = ('{} Q0 {} {} {} Exp'\n",
    "            .format(query, sorted_scores[j][0], j+1, sorted_scores[j][1]))\n",
    "        out.write(str+\"\\n\")\n",
    "        i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_out = open('./result/ml_test.txt', 'w')\n",
    "\n",
    "for test_query, ml_scores in test_pred_map.items():\n",
    "    rank_scores(test_query, ml_scores, ml_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lm.predict(train_feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_map = {}\n",
    "for query in train_queries:\n",
    "    train_pred_map[str(query)] = []\n",
    "    \n",
    "for i in range(len(train_entry_array)):\n",
    "    query, docno = train_entry_array[i].split('-', maxsplit=1)\n",
    "    train_pred_map[query].append([docno, predictions[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_out = open('./result/ml_train.txt', 'w')\n",
    "\n",
    "for train_query, ml_scores in train_pred_map.items():\n",
    "    rank_scores(train_query, ml_scores, ml_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
